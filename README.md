# LLM-As-A-Judge implementation

## Problem Statement
- There is an LLM solution expected to return function calls for a user prompt in a multi-turn conversation.
- LLM hallucinates with the current prompting techniques.
    - The hallucinations vary across domains.
    - Function names or values of parameters are constantly being invented by the LLM.
## Original Assumptions
- LLM has access to other resources.
- LLM should only use function calling when needed.
- LLM can also use its inherent knowledge and reasoning to respond directly with an answer.
- If there is a function that can return the correct value for the question, then the LLM must respond with that function call.


## Constrains
- The resources as different tools like a data store, search tool, or some grounding framework.
    - In my approach, I'll use the search tool.
- Based on the the behavior mentioned in the example I'll be using the following functions.
    - Search Internet
    - Defined Actions
- As mentioned, the search internet action/function will be a tool function that uses the search tool and answers the question.
- The defined actions are a broad category that will have sub-categories.
    - I'm assuming that the use case is around a in-car infotainment system. Hence, I'm choosing to use the following set of defined actions.
        - `Make_Call` : This action will help the users (driver/passengers) to make a call to a given number of saved contacts in their system.
        - `Call_Interaction` : This action is specific to all incoming call interactions. The users (driver/passengers) can choose to `ACCEPT`  or `REJECT`  a call.
        - `Send_Message` : This action will allow the users (driver/passengers) to send a message via the _Messages, WhatsApp, or iMessage_ app to a number or a saved contact.
        - `Audio_Interaction` : This action will allow the users (driver/passengers) to interact with their in-car entertainment system for playing audio via specific apps (YouTube Music, Spotify, etc.). They can choose to play a track, skip a track (forward/backward) or pause a track.
        - `Map_Interaction` : This action will allow the users (driver/passengers) to set/update a location they want to travel to.
- Apart from these tools, the LLM should be able to answer the question if it has any inherent knowledge about it. In that case, we'll be using an output schema like below.
```
{
  "answer": "LLMs answer to the question"
}
```
- The following will be the multi-turn conversation format.
```
[
  {
    "role": "user",
    "content": "some question"
  },
  {
    "role": "assistant",
    "content": "some answer"
  }
]
```
- For the sake of quick implementation, I'll be using a combination of models for generation and evaluation. I'll be using the newly released GPT-4o and the updated GPT-4-Turbo models.
    - I'll explain the evaluation process in the upcoming sections below.
- I'll be using Pydantic for defining function schemas.
- To demonstrate how the same can be achieved with any other LLM, I won't directly use the OpenAI Tool Calling functionality that is available out of the box with the OpenAI APIs and SDK.
    - Instead, I'll implement some prompt engineering techniques to achieve this. The same prompt engineering techniques can be applied to any other LLM (open/closed).


## Approaching this Problem
The problem of hallucination is quite real and there are multiple ways to mitigate it. One such approach can be validating the replies generated by the LLM.

### Validations/Detection
We can have the following set of validations.

1. Validating the function names generated by the LLM.
2. Validating the function schema against the one generated by the LLM.
3. In the case of open-domain questions, one can validate the LLM response against web searches.
4. In the case of responses with inherent knowledge, one can validate the format in which the answer is provided.
5. Using the LLM-as-a-Judge approach to validate and judge the LLM responses by scoring them.
In our approach, we'll have all these layers of validations.

### Mitigations
Before we reach the validation stage our completion utility should be able to mitigate these hallucinations from the start. The following is how we'll approach completion/function calling for our system to mitigate hallucinations.

1. We'll use the log probabilities provided by the LLM for the response it generated to know the confidence of the LLM in its ability to use its inherent knowledge to reply to the message/question.
    1. This basically will be a classification layer where the LLM will classify if the question can be answered with its inherent knowledge or if it needs to use some tool.
    2. If it can classify that it can answer this with the inherent knowledge stored in its weights and the confidence score calculated from its log probability is high, we'll let it answer in the format we've defined above under our assumptions section.
    3. If it classifies with a lower confidence or if it classifies that it cannot answer then we'll take the functions route described from point number "2" below.
2. The functions route/approach will first have a classification layer to choose either Search or Pre-defined actions. We'll again use the same confidence scoring technique here as well to choose the class.
3. If it chooses search then we'll use function calling to get the inputs required for the search tool in a structured manner.
4. If it chooses the pre-defined actions we'll again use function calling which allows us to use single as well as multiple function calls for the user message.
The above points will be the approach to mitigate the hallucination at the start with our completion utility.

To implement this I'll be slightly changing the mitigate hallucination function i.e. the `mitigate_hallucs`  function to take in the multi-turn conversation messages and apply the flow we discussed above to mitigate the hallucination. Hence, instead of taking the `llm_output` , it'll take the message and then apply all the layers we've discussed above and mitigate the chances of hallucination at a completion level only.



### Evaluation
Evaluating LLM responses/outputs is quite complex because of the nuanced and variable nature of responses that can be contextually grounded, grammatically incorrect, repetitive, excessively verbose, or incoherent.

So the best way I can think of as of now is to use another LLM (probably a bigger/better LLM) for judging the output generated for the user messages/requests. This approach is similar to how a human would manually rate responses but instead of using a human we're prompting another LLM to act as a human and rate the responses. Hence, this evaluation method is named as LLM-as-a-Judge.


## Final Design

The following is how the system is designed to mitigate/avoid hallucinations while text completion or generation. Validating the generated output against the function names and the schemas. And then evaluation using the **LLM-as-a-Judge** methodology.

![Design](./assets/approach-flow.png)

## Code Document

### Install Requirements

```
pip install -r requirements.txt
```

The **notebook** and the **scripts** folder contain the code for the system. 

### Notebook

The `Detect_Mitigate_Evaluate.ipynb` file in the *notebooks* folder provides an interactive way to use the system. The following are the three main functions in that file.
- `mitigateAndComplete`: Mitigating/Avoiding hallucinations while text completion.
- `verifyFunctionNameAndSchema`: Validating the output from the `mitigateAndComplete` function.
- `judgeResponse`: Evaluates the verified hallucination free output using LLM-as-a-Judge approach.

### Scripts

The code available in the `Detect_Mitigate_Evaluate.ipynb` notebooks is added to the `scripts` folder in a structured manner. The following is the tree view of the `scripts` folder.

```
.
├── configs.py
├── conversations.json
├── detect
│   ├── __init__.py
│   └── detect.py
├── evaluate.py
├── evaluation_report.json
├── judge
│   ├── __init__.py
│   ├── judge.py
│   ├── prompts.py
│   └── schema.py
├── llms
│   ├── __init__.py
│   ├── base.py
│   └── openai
│       ├── __init__.py
│       ├── __pycache__
│       │   ├── __init__.cpython-311.pyc
│       │   ├── context.cpython-311.pyc
│       │   └── llm.cpython-311.pyc
│       ├── context.py
│       └── llm.py
├── mitigate
│   ├── __init__.py
│   ├── layers.py
│   ├── mitigate.py
│   ├── prompts.py
│   ├── schemas.py
│   ├── search.py
│   └── utils.py
└── mitigatedetect.py
```

We can directly execute the `evaluate.py` file to evaluate on the $14$ records of multi-turn conversation available in the `conversations.json` file.

But before executing the `evaluate.py` file please create a `.env` file inside the `scripts` folder and add the values for the following API keys.

```
OPENAI_API_KEY="YOUR OPENAI API KEY"
BRAVE_API_KEY="YOUR BRAVE API KEY"
```

After this you can execute the `evaluate.py` using the following command.

```
python evaluate.py --is_openai --model_name gpt-4o
```
Once executed, it will create a `evaluation_report.json` file which will contain a detailed evaluation report along with the output rating for each multi-turn conversation.

## Latency and Rating

The average latency for generating a system response is between $2.5$ seconds to $2.9$ seconds.

And the average latency for evaluating a single system response is between $2.9$ seconds to $3.2$ seconds.

From the experiements that I conducted I got the average rating from $3.14$ to $3.4$. The rating is done on a scale of 1 to 4.

## Token Usage

The amount of tokens used for a multi-turn conversation depends on which capability the `mitigateAndComplete` function is invoking.
- For invoking defined functions the total tokens comsumed are in the range of $1800$ to $2200$.
- For invoking search answer the total tokens consumed are in the range of $1100$ to $1600$.
- The least amount of tokens are used when answering from inherent knowledge i.e. $550$ to $900$.

## Using Open Source LLM - `meta-llama/llama-3-70b-instruct`

As requested, I've added open source LLM for comparison. I've used the **Llama-3-70B-Instruct** model. The model is being used via the **[OpenRouter](https://openrouter.ai/)** service.

OpenRouter helps in integrating both Open source and close source LLMs via one single interface. We can directly use OpenRouter in the `llm/openai/llm.py` file. The only thing that required an update was context management and base URL for the OpenAI Python library to call the OpenRouter endpoint instead of the default OpenAI endpoint.

To keep the judge unbiased towards a set of model responses the judge is still **GPT-4-Turbo** and the average rating for the responses from the `meta-llama/llama-3-70b-instruct` LLM is $3.8$ out of $4$.

_The response times will be higher as OpenRouter is a community LLM aggregator._

> Note: As OpenRouter provides models from different providers like OpenAI, Anthropic, Fireworks.AI, etc. not all the model parameters are supported by. Hence, the `logprobs` calculation was not possible with model inference avaialble via OpenRouter. When self-hosted we can get logprobs from open source models as well.

> Note on Local LLM: I even tried using a 3 Bit quantized Llama-3-8B-Instruct model locally but the system started crashing after a couple of conversations. Hence, I had to remove that.

To run the evaluate for `meta-llama/llama-3-70b-instruct` you can use the following command.

```sh
python evaluate.py --model_name meta-llama/llama-3-70b-instruct --open_router_base_url https://openrouter.ai/api/v1
```

You can try different models from [here](https://openrouter.ai/models).

## References
- [﻿Using LLM-as-a-Judge for an automated and versatile evaluation](https://huggingface.co/learn/cookbook/en/llm_judge)﻿.
- [﻿OpenAI Cookbook: Using logprobs.](https://cookbook.openai.com/examples/using_logprobs)﻿
- [﻿Examples of using logprobs with AzureOpenAI.](https://github.com/bartczernicki/AzureOpenAILogProbs)﻿


